{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Julia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking sum_of_squares in Julia:\n",
      "\n",
      "--- Using BenchmarkTools (@btime - reports minimum time) ---\n",
      "  8.067 ms (0 allocations: 0 bytes)\n",
      "\n",
      "Benchmarking built-in sum(abs2.(data)) in Julia:\n",
      "  24.848 ms (3 allocations: 76.29 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.332693465065938e6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "using Random\n",
    "\n",
    "# Function to benchmark: sum of squares\n",
    "function sum_of_squares(arr::Vector{Float64})\n",
    "    s = 0.0\n",
    "    @inbounds for x in arr\n",
    "        s += x^2\n",
    "    end\n",
    "    return s\n",
    "end\n",
    "\n",
    "# Generate a large array for testing\n",
    "const N = 10_000_000\n",
    "data = rand(N)\n",
    "\n",
    "println(\"Benchmarking sum_of_squares in Julia:\")\n",
    "\n",
    "# --- Method 1: Using BenchmarkTools for precise minimum time (default behavior) ---\n",
    "# This is generally the recommended way for micro-benchmarking in Julia.\n",
    "# It internally runs many iterations to find a stable minimum.\n",
    "println(\"\\n--- Using BenchmarkTools (@btime - reports minimum time) ---\")\n",
    "@btime sum_of_squares($data) samples=10 # `samples` controls how many different timing runs are done\n",
    "                                      # Each \"sample\" may involve many internal iterations.\n",
    "\n",
    "\n",
    "# --- Comparing with Julia's built-in sum(abs2.(data)) ---\n",
    "println(\"\\nBenchmarking built-in sum(abs2.(data)) in Julia:\")\n",
    "@btime sum(abs2.($data)) samples=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking sum_of_squares in Julia:\n",
      "\n",
      "--- Using BenchmarkTools (@btime - reports minimum time) ---\n",
      "  8.238 ms (0 allocations: 0 bytes)\n",
      "\n",
      "Benchmarking built-in sum(abs2.(data)) in Julia:\n",
      "  21.966 ms (3 allocations: 76.29 MiB)\n",
      "\n",
      "--- Measuring total execution time including JIT (cold run) ---\n",
      "\n",
      "Timing sum_of_squares (cold run including JIT):\n",
      "\n",
      "--- Using @time for a single run (often includes JIT on first call) ---\n",
      "First run of sum_of_squares(data):\n",
      "  0.009591 seconds\n",
      "\n",
      "First run of sum(abs2.(data)):\n",
      "  0.077602 seconds (1.78 k allocations: 76.372 MiB, 15.02% gc time, 47.00% compilation time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.3357734421983166e6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "using Random\n",
    "\n",
    "# Function to benchmark: sum of squares\n",
    "function sum_of_squares(arr::Vector{Float64})\n",
    "    s = 0.0\n",
    "    @inbounds for x in arr\n",
    "        s += x^2\n",
    "    end\n",
    "    return s\n",
    "end\n",
    "\n",
    "# Generate a large array for testing\n",
    "const N = 10_000_000\n",
    "data = rand(N)\n",
    "\n",
    "println(\"Benchmarking sum_of_squares in Julia:\")\n",
    "\n",
    "# --- Method 1: Using BenchmarkTools for precise minimum time (default behavior) ---\n",
    "# This is generally the recommended way for micro-benchmarking in Julia.\n",
    "# It internally runs many iterations to find a stable minimum.\n",
    "println(\"\\n--- Using BenchmarkTools (@btime - reports minimum time) ---\")\n",
    "@btime sum_of_squares($data) samples=10 # `samples` controls how many different timing runs are done\n",
    "                                      # Each \"sample\" may involve many internal iterations.\n",
    "\n",
    "\n",
    "# --- Comparing with Julia's built-in sum(abs2.(data)) ---\n",
    "println(\"\\nBenchmarking built-in sum(abs2.(data)) in Julia:\")\n",
    "@btime sum(abs2.($data)) samples=10\n",
    "\n",
    "# --- Method 2: Measuring total execution time including JIT ---\n",
    "# To include JIT time, we need to ensure the first execution happens within the timing.\n",
    "# One way to do this is to define a helper function or block that wraps the call,\n",
    "# and then call that helper within @time or @btime for a \"cold\" run.\n",
    "\n",
    "println(\"\\n--- Measuring total execution time including JIT (cold run) ---\")\n",
    "\n",
    "# We need to ensure the function is \"cold\" before timing.\n",
    "# One way to do this is to define a new function or run it in a fresh scope.\n",
    "# For demonstration, let's redefine a simple wrapper or ensure a fresh call.\n",
    "\n",
    "function cold_sum_of_squares_run(arr)\n",
    "    sum_of_squares(arr)\n",
    "end\n",
    "\n",
    "function cold_abs2_sum_run(arr)\n",
    "    sum(abs2.(arr))\n",
    "end\n",
    "\n",
    "# Now, we time the first execution of these \"cold\" runs.\n",
    "# Note: @btime will still try to stabilize, so for a true \"single cold run\" measurement\n",
    "# @time might be more direct, but @btime with samples=1 is also an option\n",
    "# to get a single measurement that includes JIT if you ensure it's truly cold.\n",
    "\n",
    "println(\"\\nTiming sum_of_squares (cold run including JIT):\")\n",
    "# To ensure it's a truly cold run for JIT, we'll use a new, small array\n",
    "# or a fresh Julia session for the most accurate single-run JIT measurement.\n",
    "# However, within the same script, the JIT for `sum_of_squares` itself might\n",
    "# have already occurred from the @btime calls above.\n",
    "# To simulate a truly cold run for JIT, you'd typically restart Julia\n",
    "# or use a different function signature/type.\n",
    "# For illustrative purposes *within this script*, we'll time the *first*\n",
    "# call to a function that *might* not have been JIT-ed for these specific types yet,\n",
    "# or simply acknowledge that subsequent runs will be warm.\n",
    "\n",
    "# A more robust way to truly measure JIT for a function is to run it in a new process,\n",
    "# but for a quick measurement within the same session:\n",
    "# You can define a function that itself calls sum_of_squares with a slightly different\n",
    "# approach or type to force a fresh JIT.\n",
    "# Or, simply time the very first call to sum_of_squares with @time outside of @btime's\n",
    "# warm-up cycles.\n",
    "\n",
    "# Let's use `@time` for a single, direct measurement that includes JIT.\n",
    "# To ensure a \"cold\" run for this specific measurement, we can define an anonymous\n",
    "# function or wrap it in a `begin...end` block, and then time its *first* execution.\n",
    "# However, given that `sum_of_squares($data)` has already been called by `@btime`,\n",
    "# the JIT for `sum_of_squares` with `Vector{Float64}` has likely already occurred.\n",
    "\n",
    "# The most reliable way to measure JIT time for a function is to run it\n",
    "# in a fresh Julia process, or to ensure that the function has not been called\n",
    "# with the specific argument types before.\n",
    "\n",
    "# For demonstrating the concept *within a single script run*:\n",
    "# Let's create a *new function* or *new type signature* that hasn't been JIT'd yet.\n",
    "# This is a bit contrived for `sum_of_squares`, but illustrates the point.\n",
    "\n",
    "# Instead, the simplest approach for \"cold\" timing is often to simply use `@time`\n",
    "# on the very first execution of the code in a *new Julia session*.\n",
    "\n",
    "# If you want to include JIT time for `sum_of_squares($data)` within this script,\n",
    "# and you know it's the *first* time it's called with `Vector{Float64}`:\n",
    "\n",
    "println(\"\\n--- Using @time for a single run (often includes JIT on first call) ---\")\n",
    "println(\"First run of sum_of_squares(data):\")\n",
    "@time sum_of_squares(data); # The semicolon suppresses output of the return value\n",
    "\n",
    "println(\"\\nFirst run of sum(abs2.(data)):\")\n",
    "@time sum(abs2.(data));\n",
    "\n",
    "# Important Note: If you run the entire script, the `@btime` calls for `sum_of_squares`\n",
    "# and `sum(abs2.(data))` will likely JIT compile them first.\n",
    "# Therefore, the subsequent `@time` calls in the same script run will measure a *warm* execution,\n",
    "# not a cold one including JIT.\n",
    "\n",
    "# To *truly* measure JIT time for `sum_of_squares(data)`:\n",
    "# 1. Restart your Julia session.\n",
    "# 2. Run only these lines:\n",
    "#    using Random\n",
    "#    const N = 10_000_000\n",
    "#    data = rand(N)\n",
    "#    function sum_of_squares(arr::Vector{Float64})\n",
    "#        s = 0.0\n",
    "#        @inbounds for x in arr\n",
    "#            s += x^2\n",
    "#        end\n",
    "#        return s\n",
    "#    end\n",
    "#    println(\"Timing sum_of_squares (cold run including JIT):\")\n",
    "#    @time sum_of_squares(data);\n",
    "#\n",
    "# This `times` the very first execution of `sum_of_squares` for `Vector{Float64}`,\n",
    "# which will include the JIT compilation time.\n",
    "\n",
    "# Similarly for `sum(abs2.(data))`:\n",
    "# 1. Restart your Julia session.\n",
    "# 2. Run these lines:\n",
    "#    using Random\n",
    "#    const N = 10_000_000\n",
    "#    data = rand(N)\n",
    "#    println(\"Timing sum(abs2.(data)) (cold run including JIT):\")\n",
    "#    @time sum(abs2.(data));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.9.16 (main, Mar  8 2023, 10:39:24) [MSC v.1916 64 bit (AMD64)]\n",
      "Benchmarking sum_of_squares in Python:\n",
      "Time for single run: 986.678700 milliseconds\n",
      "Average time over 10 repetitions: 1020.809320 milliseconds\n",
      "Min time: 997.342300 milliseconds\n",
      "Median time: 1020.354100 milliseconds\n",
      "\n",
      "Benchmarking with NumPy's sum(data**2) in Python:\n",
      "Average NumPy time over 10 repetitions: 38.569510 milliseconds\n",
      "Min NumPy time: 36.750200 milliseconds\n",
      "Median NumPy time: 37.739850 milliseconds\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import random\n",
    "import statistics\n",
    "import sys # For sys.version\n",
    "\n",
    "# Function to benchmark: sum of squares\n",
    "def sum_of_squares(arr):\n",
    "    s = 0.0\n",
    "    for x in arr:\n",
    "        s += x**2\n",
    "    return s\n",
    "\n",
    "# Generate a large list for testing\n",
    "N = 10_000_000\n",
    "data = [random.random() for _ in range(N)]\n",
    "\n",
    "print(f\"Python Version: {sys.version.splitlines()[0]}\")\n",
    "print(\"Benchmarking sum_of_squares in Python:\")\n",
    "\n",
    "num_repetitions = 10 # Number of times to repeat the entire measurement process\n",
    "num_executions_per_run = 1 # Number of times the statement is executed within one timed run\n",
    "\n",
    "# Setup code is crucial for timeit to create the data in the timing context\n",
    "setup_code = \"\"\"\n",
    "import random\n",
    "N = 10_000_000\n",
    "data = [random.random() for _ in range(N)]\n",
    "\"\"\"\n",
    "stmt_code = \"sum_of_squares(data)\"\n",
    "\n",
    "# --- Manual adjustment for 'timing_result' output ---\n",
    "# Using timeit.timeit to get a single timing\n",
    "# It automatically determines a good number of iterations\n",
    "# Changed number to 1 as we want 1 run per \"repetition\" for consistency with `repeat` later\n",
    "# And multiplied by 1000 for milliseconds\n",
    "timing_result_seconds = timeit.timeit(stmt=stmt_code, setup=setup_code, globals=globals(), number=num_executions_per_run)\n",
    "print(f\"Time for single run: {timing_result_seconds * 1000:.6f} milliseconds\")\n",
    "\n",
    "\n",
    "# --- Using timeit.repeat for more robust measurements (adjusted for milliseconds) ---\n",
    "# It runs the timing multiple times and returns a list of results\n",
    "# We'll run it 'num_repetitions' times, with each run executing the statement 'num_executions_per_run' time\n",
    "repeat_results_seconds = timeit.repeat(\n",
    "    stmt=stmt_code,\n",
    "    setup=setup_code,\n",
    "    globals=globals(), # Allows access to sum_of_squares in the current global scope\n",
    "    number=num_executions_per_run, # Execute the statement once per timed run\n",
    "    repeat=num_repetitions # Repeat the whole process 'num_repetitions' times\n",
    ")\n",
    "\n",
    "# Convert all results to milliseconds\n",
    "repeat_results_ms = [t * 1000 for t in repeat_results_seconds]\n",
    "\n",
    "# print(f\"Individual times over {num_repetitions} repetitions: {[f'{t:.6f}' for t in repeat_results_ms]} milliseconds\")\n",
    "average_time_ms = statistics.mean(repeat_results_ms)\n",
    "print(f\"Average time over {num_repetitions} repetitions: {average_time_ms:.6f} milliseconds\")\n",
    "print(f\"Min time: {min(repeat_results_ms):.6f} milliseconds\")\n",
    "print(f\"Median time: {statistics.median(repeat_results_ms):.6f} milliseconds\")\n",
    "\n",
    "\n",
    "# Comparing with NumPy (highly optimized C/Fortran backend) - adjusted for milliseconds\n",
    "try:\n",
    "    import numpy as np\n",
    "    # It's better to create numpy_data outside the setup string if it's the same for all runs\n",
    "    # and passed via globals, to avoid recreating it for each measurement in timeit.repeat\n",
    "    numpy_data = np.random.rand(N)\n",
    "\n",
    "    print(\"\\nBenchmarking with NumPy's sum(data**2) in Python:\")\n",
    "    numpy_stmt = \"np.sum(numpy_data**2)\"\n",
    "    numpy_setup = f\"\"\"\n",
    "import numpy as np\n",
    "N = {N}\n",
    "# Re-create numpy_data here to ensure data generation is part of the benchmark setup for each run\n",
    "numpy_data = np.random.rand(N)\n",
    "    \"\"\"\n",
    "\n",
    "    numpy_repeat_results_seconds = timeit.repeat(\n",
    "        stmt=numpy_stmt,\n",
    "        setup=numpy_setup,\n",
    "        globals=globals(),\n",
    "        number=num_executions_per_run,\n",
    "        repeat=num_repetitions\n",
    "    )\n",
    "\n",
    "    # Convert NumPy results to milliseconds\n",
    "    numpy_repeat_results_ms = [t * 1000 for t in numpy_repeat_results_seconds]\n",
    "\n",
    "    # print(f\"Individual NumPy times over {num_repetitions} repetitions: {[f'{t:.6f}' for t in numpy_repeat_results_ms]} milliseconds\")\n",
    "    numpy_average_time_ms = statistics.mean(numpy_repeat_results_ms)\n",
    "    print(f\"Average NumPy time over {num_repetitions} repetitions: {numpy_average_time_ms:.6f} milliseconds\")\n",
    "    print(f\"Min NumPy time: {min(numpy_repeat_results_ms):.6f} milliseconds\")\n",
    "    print(f\"Median NumPy time: {statistics.median(numpy_repeat_results_ms):.6f} milliseconds\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\nNumPy not installed. Skipping NumPy benchmark.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.9.16 (main, Mar  8 2023, 10:39:24) [MSC v.1916 64 bit (AMD64)]\n",
      "Benchmarking sum_of_squares in Python:\n",
      "\n",
      "--- Measuring 'warm' execution (function defined, data generated in setup) ---\n",
      "Average 'warm' time over 10 repetitions: 1024.611740 milliseconds\n",
      "Min 'warm' time: 984.193500 milliseconds\n",
      "Median 'warm' time: 1023.253450 milliseconds\n",
      "\n",
      "--- Measuring 'cold' execution (function definition and data generation included in stmt) ---\n",
      "Average 'cold' time over 10 repetitions: 2077.555100 milliseconds\n",
      "Min 'cold' time: 1916.075200 milliseconds\n",
      "Median 'cold' time: 2062.781100 milliseconds\n",
      "\n",
      "Benchmarking with NumPy's sum(data**2) in Python:\n",
      "Average NumPy 'warm' time over 10 repetitions: 39.611930 milliseconds\n",
      "Min NumPy 'warm' time: 38.120900 milliseconds\n",
      "Median NumPy 'warm' time: 39.023500 milliseconds\n",
      "\n",
      "--- Measuring NumPy 'cold' execution (import and data generation included in stmt) ---\n",
      "Average NumPy 'cold' time over 10 repetitions: 108.636610 milliseconds\n",
      "Min NumPy 'cold' time: 102.765500 milliseconds\n",
      "Median NumPy 'cold' time: 108.021500 milliseconds\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import random\n",
    "import statistics\n",
    "import sys # For sys.version\n",
    "\n",
    "# Function to benchmark: sum of squares\n",
    "def sum_of_squares(arr):\n",
    "    s = 0.0\n",
    "    for x in arr:\n",
    "        s += x**2\n",
    "    return s\n",
    "\n",
    "# Generate a large list for testing - This is done outside timeit for initial demo\n",
    "N = 10_000_000\n",
    "data = [random.random() for _ in range(N)]\n",
    "\n",
    "print(f\"Python Version: {sys.version.splitlines()[0]}\")\n",
    "print(\"Benchmarking sum_of_squares in Python:\")\n",
    "\n",
    "num_repetitions = 10 # Number of times to repeat the entire measurement process\n",
    "num_executions_per_run = 1 # Number of times the statement is executed within one timed run\n",
    "\n",
    "# --- Scenario 1: Measuring \"warm\" execution (typical `timeit` use) ---\n",
    "# The setup code includes the data generation, but `timeit` runs the setup\n",
    "# once per repetition. The `sum_of_squares` function is already defined.\n",
    "print(\"\\n--- Measuring 'warm' execution (function defined, data generated in setup) ---\")\n",
    "\n",
    "setup_code_warm = \"\"\"\n",
    "import random\n",
    "N = 10_000_000\n",
    "data = [random.random() for _ in range(N)]\n",
    "from __main__ import sum_of_squares # Import the function from the global scope\n",
    "\"\"\"\n",
    "stmt_code = \"sum_of_squares(data)\"\n",
    "\n",
    "# Using timeit.repeat for robust measurements\n",
    "repeat_results_seconds_warm = timeit.repeat(\n",
    "    stmt=stmt_code,\n",
    "    setup=setup_code_warm,\n",
    "    number=num_executions_per_run, # Execute the statement once per timed run\n",
    "    repeat=num_repetitions        # Repeat the whole process 'num_repetitions' times\n",
    ")\n",
    "\n",
    "# Convert all results to milliseconds\n",
    "repeat_results_ms_warm = [t * 1000 for t in repeat_results_seconds_warm]\n",
    "\n",
    "average_time_ms_warm = statistics.mean(repeat_results_ms_warm)\n",
    "print(f\"Average 'warm' time over {num_repetitions} repetitions: {average_time_ms_warm:.6f} milliseconds\")\n",
    "print(f\"Min 'warm' time: {min(repeat_results_ms_warm):.6f} milliseconds\")\n",
    "print(f\"Median 'warm' time: {statistics.median(repeat_results_ms_warm):.6f} milliseconds\")\n",
    "\n",
    "\n",
    "# --- Scenario 2: Measuring \"cold\" execution (simulating initial load/setup) ---\n",
    "# To include the \"overhead\" of defining the function and creating the data,\n",
    "# we put everything into the `stmt` and use a minimal `setup`.\n",
    "# This is as close as you get to a \"total execution time including initial setup\" in Python.\n",
    "print(\"\\n--- Measuring 'cold' execution (function definition and data generation included in stmt) ---\")\n",
    "\n",
    "setup_code_cold = \"\"\"\n",
    "import random\n",
    "\"\"\"\n",
    "stmt_code_cold = f\"\"\"\n",
    "def sum_of_squares_cold(arr): # Renamed to avoid conflicts if `sum_of_squares` is already in globals\n",
    "    s = 0.0\n",
    "    for x in arr:\n",
    "        s += x**2\n",
    "    return s\n",
    "\n",
    "N = {N}\n",
    "data_cold = [random.random() for _ in range(N)]\n",
    "sum_of_squares_cold(data_cold)\n",
    "\"\"\"\n",
    "\n",
    "# Using timeit.repeat\n",
    "repeat_results_seconds_cold = timeit.repeat(\n",
    "    stmt=stmt_code_cold,\n",
    "    setup=setup_code_cold,\n",
    "    number=num_executions_per_run, # Execute the entire statement once per timed run\n",
    "    repeat=num_repetitions        # Repeat the whole process 'num_repetitions' times\n",
    ")\n",
    "\n",
    "# Convert all results to milliseconds\n",
    "repeat_results_ms_cold = [t * 1000 for t in repeat_results_seconds_cold]\n",
    "\n",
    "average_time_ms_cold = statistics.mean(repeat_results_ms_cold)\n",
    "print(f\"Average 'cold' time over {num_repetitions} repetitions: {average_time_ms_cold:.6f} milliseconds\")\n",
    "print(f\"Min 'cold' time: {min(repeat_results_ms_cold):.6f} milliseconds\")\n",
    "print(f\"Median 'cold' time: {statistics.median(repeat_results_ms_cold):.6f} milliseconds\")\n",
    "\n",
    "\n",
    "# --- Comparing with NumPy (highly optimized C/Fortran backend) ---\n",
    "try:\n",
    "    import numpy as np\n",
    "\n",
    "    print(\"\\nBenchmarking with NumPy's sum(data**2) in Python:\")\n",
    "\n",
    "    # Scenario 3: NumPy \"warm\" execution\n",
    "    # numpy_data creation is in setup, np.sum is in stmt\n",
    "    numpy_setup_warm = f\"\"\"\n",
    "import numpy as np\n",
    "N = {N}\n",
    "numpy_data = np.random.rand(N)\n",
    "    \"\"\"\n",
    "    numpy_stmt_warm = \"np.sum(numpy_data**2)\"\n",
    "\n",
    "    numpy_repeat_results_seconds_warm = timeit.repeat(\n",
    "        stmt=numpy_stmt_warm,\n",
    "        setup=numpy_setup_warm,\n",
    "        number=num_executions_per_run,\n",
    "        repeat=num_repetitions\n",
    "    )\n",
    "\n",
    "    numpy_repeat_results_ms_warm = [t * 1000 for t in numpy_repeat_results_seconds_warm]\n",
    "\n",
    "    numpy_average_time_ms_warm = statistics.mean(numpy_repeat_results_ms_warm)\n",
    "    print(f\"Average NumPy 'warm' time over {num_repetitions} repetitions: {numpy_average_time_ms_warm:.6f} milliseconds\")\n",
    "    print(f\"Min NumPy 'warm' time: {min(numpy_repeat_results_ms_warm):.6f} milliseconds\")\n",
    "    print(f\"Median NumPy 'warm' time: {statistics.median(numpy_repeat_results_ms_warm):.6f} milliseconds\")\n",
    "\n",
    "    # Scenario 4: NumPy \"cold\" execution (includes import and data generation in stmt)\n",
    "    print(\"\\n--- Measuring NumPy 'cold' execution (import and data generation included in stmt) ---\")\n",
    "    numpy_setup_cold = \"\" # No setup needed for this case\n",
    "    numpy_stmt_cold = f\"\"\"\n",
    "import numpy as np\n",
    "N = {N}\n",
    "numpy_data_cold = np.random.rand(N)\n",
    "np.sum(numpy_data_cold**2)\n",
    "    \"\"\"\n",
    "\n",
    "    numpy_repeat_results_seconds_cold = timeit.repeat(\n",
    "        stmt=numpy_stmt_cold,\n",
    "        setup=numpy_setup_cold,\n",
    "        number=num_executions_per_run,\n",
    "        repeat=num_repetitions\n",
    "    )\n",
    "\n",
    "    numpy_repeat_results_ms_cold = [t * 1000 for t in numpy_repeat_results_seconds_cold]\n",
    "\n",
    "    numpy_average_time_ms_cold = statistics.mean(numpy_repeat_results_ms_cold)\n",
    "    print(f\"Average NumPy 'cold' time over {num_repetitions} repetitions: {numpy_average_time_ms_cold:.6f} milliseconds\")\n",
    "    print(f\"Min NumPy 'cold' time: {min(numpy_repeat_results_ms_cold):.6f} milliseconds\")\n",
    "    print(f\"Median NumPy 'cold' time: {statistics.median(numpy_repeat_results_ms_cold):.6f} milliseconds\")\n",
    "\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\nNumPy not installed. Skipping NumPy benchmark.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "package ‘microbenchmark’ was built under R version 4.4.3 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking sum_of_squares in R (using microbenchmark):\n",
      "Unit: milliseconds\n",
      "         expr      min       lq      mean   median       uq      max neval\n",
      "  custom_loop 269.3599 273.3218 280.03448 277.8791 287.5356 291.0119    10\n",
      " r_vectorized  34.1022  34.7135  72.27134  50.4263  51.6792 327.3412    10\n"
     ]
    }
   ],
   "source": [
    "# Function to benchmark: sum of squares\n",
    "sum_of_squares <- function(arr) {\n",
    "  s <- 0.0\n",
    "  for (x in arr) {\n",
    "    s <- s + x^2\n",
    "  }\n",
    "  return(s)\n",
    "}\n",
    "\n",
    "# Generate a large vector for testing\n",
    "N <- 10000000 # Corrected: No commas or underscores in numeric literals in R\n",
    "data <- runif(N) # Generates random numbers from a uniform distribution\n",
    "\n",
    "# # --- Using system.time() ---\n",
    "# cat(\"Benchmarking sum_of_squares in R (using system.time):\\n\")\n",
    "# num_runs <- 5\n",
    "# timings_ms <- numeric(num_runs) # Store timings in milliseconds\n",
    "\n",
    "# # Warm-up run (optional, but good practice for ensuring JIT compilation is done)\n",
    "# sum_of_squares(data)\n",
    "\n",
    "# for (i in 1:num_runs) {\n",
    "#   timing_result <- system.time(sum_of_squares(data))\n",
    "#   timings_ms[i] <- timing_result[\"elapsed\"] * 1000 # Convert seconds to milliseconds\n",
    "# }\n",
    "# cat(paste0(\"Average elapsed time over \", num_runs, \" runs: \", sprintf(\"%.6f\", mean(timings_ms)), \" milliseconds\\n\"))\n",
    "\n",
    "# --- Using microbenchmark package ---\n",
    "# Install if you don't have it: install.packages(\"microbenchmark\")\n",
    "if (!requireNamespace(\"microbenchmark\", quietly = TRUE)) {\n",
    "  install.packages(\"microbenchmark\")\n",
    "}\n",
    "library(microbenchmark)\n",
    "\n",
    "cat(\"\\nBenchmarking sum_of_squares in R (using microbenchmark):\\n\")\n",
    "num_repetitions <- 10 # Number of times to evaluate each expression\n",
    "\n",
    "mb_results <- microbenchmark(\n",
    "  custom_loop = sum_of_squares(data),\n",
    "  r_vectorized = sum(data^2), # R's vectorized operation\n",
    "  times = num_repetitions # Number of times to evaluate each expression\n",
    ")\n",
    "\n",
    "# Print microbenchmark results. By default, it uses the most appropriate unit (ns, us, ms).\n",
    "# We'll then explicitly extract and print the mean in milliseconds below.\n",
    "print(mb_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Measuring 'Warm' Execution Time (using microbenchmark) ---\n",
      "Unit: milliseconds\n",
      "         expr      min       lq      mean   median       uq      max neval\n",
      "  custom_loop 278.7535 279.9589 287.73448 282.9196 293.9198 312.6514    10\n",
      " r_vectorized  34.6153  35.1832  73.40119  36.4012  53.6153 374.1119    10\n",
      "\n",
      "Warm Run Results (Mean from microbenchmark):\n",
      "custom_loop: 0.000288 milliseconds\n",
      "r_vectorized: 0.000073 milliseconds\n",
      "\n",
      "--- Measuring 'Cold' Execution Time (using system.time for a single, initial call) ---\n",
      "Cold run of a newly defined sum_of_squares_cold function:\n",
      "Cold run (new function + data gen): 400.000000 milliseconds (elapsed)\n",
      "\n",
      "Cold run of R's vectorized sum(data^2) (including data generation):\n",
      "Cold run (vectorized + data gen): 170.000000 milliseconds (elapsed)\n"
     ]
    }
   ],
   "source": [
    "# Function to benchmark: sum of squares\n",
    "sum_of_squares <- function(arr) {\n",
    "  s <- 0.0\n",
    "  for (x in arr) {\n",
    "    s <- s + x^2\n",
    "  }\n",
    "  return(s)\n",
    "}\n",
    "\n",
    "# Generate a large vector for testing\n",
    "N <- 10000000\n",
    "data <- runif(N)\n",
    "\n",
    "# --- Measuring \"Warm\" Execution Time (using microbenchmark package) ---\n",
    "# This package is designed for precise micro-benchmarking, typically excluding\n",
    "# initial compilation/setup overhead by warming up.\n",
    "if (!requireNamespace(\"microbenchmark\", quietly = TRUE)) {\n",
    "  install.packages(\"microbenchmark\")\n",
    "}\n",
    "library(microbenchmark)\n",
    "\n",
    "cat(\"\\n--- Measuring 'Warm' Execution Time (using microbenchmark) ---\\n\")\n",
    "num_repetitions <- 10 # Number of times to evaluate each expression\n",
    "\n",
    "mb_results <- microbenchmark(\n",
    "  custom_loop = sum_of_squares(data),\n",
    "  r_vectorized = sum(data^2), # R's vectorized operation\n",
    "  times = num_repetitions # Number of times to evaluate each expression\n",
    ")\n",
    "\n",
    "# Print microbenchmark results.\n",
    "# We'll then explicitly extract and print the mean in milliseconds below.\n",
    "print(mb_results)\n",
    "\n",
    "# Extract and print mean times in milliseconds for clarity\n",
    "cat(\"\\nWarm Run Results (Mean from microbenchmark):\\n\")\n",
    "summary_mb <- summary(mb_results)\n",
    "for (i in 1:nrow(summary_mb)) {\n",
    "  cat(sprintf(\"%s: %.6f milliseconds\\n\", summary_mb$expr[i], summary_mb$mean[i] / 1e6)) # Convert nanoseconds to milliseconds\n",
    "}\n",
    "\n",
    "\n",
    "# --- Measuring \"Cold\" Execution Time (including initial byte-code compilation) ---\n",
    "# To include the \"cold start\" (byte-code compilation, function loading, data loading),\n",
    "# we need to ensure the function is executed for the *very first time* within our timing.\n",
    "# The most direct way is to use system.time() on a fresh call.\n",
    "\n",
    "cat(\"\\n--- Measuring 'Cold' Execution Time (using system.time for a single, initial call) ---\\n\")\n",
    "\n",
    "# To ensure a truly cold run for `sum_of_squares` within the same R session,\n",
    "# we need to ensure the function definition itself and its execution hasn't been\n",
    "# previously compiled/cached by R's byte-code compiler for these specific inputs.\n",
    "# A common trick is to redefine the function or run it in a new environment,\n",
    "# or simply ensure it's the absolute first call of that specific function with those types.\n",
    "\n",
    "# For the purpose of demonstrating \"cold\" time within the same script,\n",
    "# we'll create a temporary, locally defined function to guarantee it's being\n",
    "# compiled for the first time *when timed*.\n",
    "\n",
    "# Scenario 1: Measuring the cold start of a *newly defined function*\n",
    "# (This simulates a truly cold run more closely than just calling sum_of_squares again)\n",
    "cat(\"Cold run of a newly defined sum_of_squares_cold function:\\n\")\n",
    "# We also generate data *inside* the timing if we want to include that overhead\n",
    "# for a complete \"cold start\" scenario.\n",
    "cold_time_result_1 <- system.time({\n",
    "  # Define the function locally to ensure it's \"new\" for R's compiler\n",
    "  sum_of_squares_cold_1 <- function(arr_cold) {\n",
    "    s_cold <- 0.0\n",
    "    for (x_cold in arr_cold) {\n",
    "      s_cold <- s_cold + x_cold^2\n",
    "    }\n",
    "    return(s_cold)\n",
    "  }\n",
    "  N_cold_1 <- 10000000\n",
    "  data_cold_1 <- runif(N_cold_1)\n",
    "  sum_of_squares_cold_1(data_cold_1)\n",
    "})\n",
    "cat(sprintf(\"Cold run (new function + data gen): %.6f milliseconds (elapsed)\\n\", cold_time_result_1[\"elapsed\"] * 1000))\n",
    "\n",
    "\n",
    "# Scenario 2: Measuring the cold start of R's vectorized operation\n",
    "cat(\"\\nCold run of R's vectorized sum(data^2) (including data generation):\\n\")\n",
    "cold_time_result_2 <- system.time({\n",
    "  N_cold_2 <- 10000000\n",
    "  data_cold_2 <- runif(N_cold_2)\n",
    "  sum(data_cold_2^2)\n",
    "})\n",
    "cat(sprintf(\"Cold run (vectorized + data gen): %.6f milliseconds (elapsed)\\n\", cold_time_result_2[\"elapsed\"] * 1000))\n",
    "\n",
    "# Important Note:\n",
    "# If you just ran `system.time(sum_of_squares(data))` multiple times without\n",
    "# redefining `sum_of_squares` or using a fresh R session, only the very first\n",
    "# `system.time` call would potentially include the initial byte-code compilation\n",
    "# for `sum_of_squares`. Subsequent calls would be \"warm.\"\n",
    "\n",
    "# For a truly isolated \"cold start\" measurement (especially for small functions or scripts),\n",
    "# the most robust way is to run the R code in a fresh R process each time,\n",
    "# or to use a tool that specifically manages process isolation for benchmarking.\n",
    "# Within a single R script, defining functions locally within the timed block\n",
    "# is the best approximation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
